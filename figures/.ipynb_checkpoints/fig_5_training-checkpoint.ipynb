{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "from models import pmVAEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_annotations\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/anaconda3/envs/new_torch/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3331: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "## expression and IHC data\n",
    "\n",
    "rna_seq = pd.read_csv('data/alzheimers-disease-gene-modules-master-data-preprocessed_data/data/preprocessed_data/All_Datasets_Preprocessed_Expression_Matrix.tsv',sep='\\t',index_col=0)\n",
    "abeta_data = pd.read_csv('data/alzheimers-disease-gene-modules-master-data-preprocessed_data/data/preprocessed_data/All_Datasets_Abeta_IHC_Labels.tsv',index_col=0,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta data labels \n",
    "dataset_labels = pd.read_csv('data/alzheimers-disease-gene-modules-master-data-preprocessed_data/data/preprocessed_data/All_Dataset_Labels.tsv',index_col=0,sep='\\t')\n",
    "region_labels = pd.read_csv('data/alzheimers-disease-gene-modules-master-data-preprocessed_data/data/preprocessed_data/All_Joined_Region_Labels.tsv',index_col=0,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# filter microarray data\n",
    "microarray = dataset_labels['Dataset Label'] == \"MSBB_MICROARRAY\"\n",
    "X = rna_seq.loc[~microarray.values,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_mat = load_annotations(\n",
    "    'data/h.all.v7.4.symbols.gmt',\n",
    "    X.columns,\n",
    "    min_genes=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "membership_mask = assignment_mat.astype(bool).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we should probably clean this up and put it in its own importable file\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class RNASeqData(Dataset):\n",
    "    \n",
    "    def __init__(self, X, c=None, y=None, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.c = c\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.X[index,:]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        if self.y is not None and self.c is not None:\n",
    "            return sample, self.y[index], self.c[index]\n",
    "        if self.y is None and self.c is not None:\n",
    "            return sample, self.c[index]\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train models with no dataset / region conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADVAE = pmVAEModel(\n",
    "        membership_mask.values,\n",
    "        [12],\n",
    "        4,\n",
    "        beta=1e-05,\n",
    "        terms=membership_mask.index,\n",
    "        add_auxiliary_module=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Epoch 000 --------\n",
      "Epoch 000: training loss 13044.4909,validation loss 11758.1640\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11848.1270,validation loss 11224.8703\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11475.5509,validation loss 10974.6418\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11279.4770,validation loss 10830.1279\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 11109.2604,validation loss 10701.8593\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10947.4038,validation loss 10499.7571\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10724.9951,validation loss 10245.5490\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10547.9829,validation loss 10147.5860\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10355.2398,validation loss 9974.7240\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 10175.1927,validation loss 9777.4967\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 10019.7977,validation loss 9610.5334\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9842.1952,validation loss 9477.5749\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9718.6499,validation loss 9336.6436\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9587.8819,validation loss 9320.7677\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9538.0117,validation loss 9183.7974\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9434.1838,validation loss 9147.8361\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9356.0111,validation loss 9025.3891\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9281.4428,validation loss 8952.0342\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9198.1300,validation loss 8952.7708\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9095.5120,validation loss 8901.3537\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9158.8322,validation loss 8890.6545\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9088.3888,validation loss 8859.0072\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9080.1668,validation loss 8866.4815\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9103.4618,validation loss 8911.9609\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9103.3615,validation loss 8884.0025\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9072.8049,validation loss 8862.7048\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9126.4736,validation loss 9018.6129\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9104.8195,validation loss 8985.4855\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9050.8855,validation loss 8869.8876\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9097.1744,validation loss 8851.0981\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9094.6663,validation loss 8890.8358\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9089.0417,validation loss 8884.8775\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9068.6438,validation loss 8833.8481\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9112.3210,validation loss 8906.1790\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9090.3018,validation loss 8911.6767\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9085.1885,validation loss 8911.7419\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9092.1774,validation loss 8854.4398\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9085.2465,validation loss 8885.6874\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9089.8287,validation loss 8869.3778\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9090.1672,validation loss 8876.6860\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9083.9898,validation loss 8880.4144\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9097.7044,validation loss 8841.5817\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9095.9230,validation loss 8958.9808\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9113.7077,validation loss 8877.8726\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9085.8000,validation loss 8884.4797\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9108.3761,validation loss 8858.8447\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9108.3516,validation loss 8886.3160\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9089.6384,validation loss 8863.8580\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9078.2145,validation loss 8853.4337\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9089.2278,validation loss 8901.7496\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13414.3885,validation loss 11458.5587\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11987.9965,validation loss 10534.8994\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11355.8781,validation loss 10171.3553\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11040.5933,validation loss 10025.5901\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10792.4484,validation loss 9767.3522\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10628.1992,validation loss 9622.9839\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10459.1669,validation loss 9516.8341\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10324.8885,validation loss 9439.0096\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10207.7070,validation loss 9209.6856\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 10023.6496,validation loss 9045.2037\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9829.9412,validation loss 8888.7950\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9734.9981,validation loss 8785.7590\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9632.2397,validation loss 8721.2852\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9589.4373,validation loss 8620.7146\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9487.8536,validation loss 8578.1806\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9419.5826,validation loss 8495.8210\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9343.5461,validation loss 8498.6002\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9263.4443,validation loss 8487.3566\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9340.0697,validation loss 8510.5468\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9287.9626,validation loss 8439.9820\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9266.6458,validation loss 8453.5424\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9294.4044,validation loss 8466.3404\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9252.9055,validation loss 8471.4992\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9252.5857,validation loss 8449.1252\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9261.5952,validation loss 8441.3724\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9251.4224,validation loss 8483.9252\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9255.6425,validation loss 8480.8571\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9261.0043,validation loss 8442.5832\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9286.9589,validation loss 8484.3916\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9297.5643,validation loss 8428.0293\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9269.4223,validation loss 8456.0658\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9258.6612,validation loss 8486.4617\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9304.0764,validation loss 8455.1596\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9286.7356,validation loss 8461.8561\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9260.4892,validation loss 8481.1814\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9252.8347,validation loss 8443.6655\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9244.4966,validation loss 8469.7426\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9298.3265,validation loss 8457.5193\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9263.8439,validation loss 8486.7844\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9250.8939,validation loss 8495.6934\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9245.1198,validation loss 8480.5286\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9267.4523,validation loss 8510.6416\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9246.4837,validation loss 8425.1935\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9270.5777,validation loss 8478.8637\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9256.9056,validation loss 8446.1597\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9258.1446,validation loss 8421.7332\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9258.1633,validation loss 8464.5466\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9248.9018,validation loss 8424.1381\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9276.5614,validation loss 8588.8850\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9238.0681,validation loss 8438.9012\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 12754.1414,validation loss 189632.9277\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11468.4755,validation loss 12136.2801\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11126.8861,validation loss 11779.7686\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 10910.1636,validation loss 11614.1078\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10720.3835,validation loss 11375.3963\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10446.5853,validation loss 11125.0442\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10147.3515,validation loss 10815.5106\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 9971.7187,validation loss 10656.7984\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 9850.2109,validation loss 10535.2853\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9718.5004,validation loss 10390.5658\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9620.9289,validation loss 10122.1629\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9471.7765,validation loss 9934.7070\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9285.0010,validation loss 9746.6001\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9207.0533,validation loss 9605.9504\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9115.8567,validation loss 9498.9448\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9049.9998,validation loss 9424.1696\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9020.8194,validation loss 9425.9752\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 8934.5520,validation loss 9353.3811\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 8892.6077,validation loss 9363.2539\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 8893.7647,validation loss 9429.3641\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 8897.2360,validation loss 9294.7579\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 8917.1048,validation loss 9375.5140\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 8890.1095,validation loss 9359.8985\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 8924.8864,validation loss 9327.4675\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 8902.2791,validation loss 9363.0527\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 8907.3801,validation loss 9390.7788\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 8901.7392,validation loss 9354.0299\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 8892.3030,validation loss 9361.6314\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 8903.2096,validation loss 9364.6686\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 8888.4128,validation loss 9363.1342\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 8923.0775,validation loss 9345.0135\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 8895.1235,validation loss 9374.8940\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 8917.7799,validation loss 9341.7888\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 8908.4101,validation loss 9391.5541\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 8886.4052,validation loss 9387.2538\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 8915.2133,validation loss 9372.7274\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 8885.5649,validation loss 9308.0866\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 8920.7864,validation loss 9378.4630\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 8912.5877,validation loss 9356.4846\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 8910.5613,validation loss 9364.4440\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 8896.2050,validation loss 9386.8247\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 8907.3383,validation loss 9367.0638\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 8917.3786,validation loss 9347.5196\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 8923.8715,validation loss 9330.0314\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 8941.0135,validation loss 9398.1748\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 8914.7598,validation loss 9338.2077\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 8916.8630,validation loss 9388.7636\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 8969.7350,validation loss 9354.2320\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 8929.6137,validation loss 9358.0580\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 8913.5510,validation loss 9398.7061\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13056.9705,validation loss 12003.1018\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11823.6567,validation loss 11370.2397\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11439.7336,validation loss 11125.6955\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11177.4734,validation loss 10920.2947\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 11025.5453,validation loss 10784.8363\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10910.0432,validation loss 10667.9821\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10769.2143,validation loss 10500.8307\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10647.0084,validation loss 10403.5592\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10492.0205,validation loss 10201.3572\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 10263.1052,validation loss 9974.5325\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 10043.0760,validation loss 9762.9674\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9925.6629,validation loss 9707.5452\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9835.7010,validation loss 9619.7417\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9710.9706,validation loss 9421.6433\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9625.1943,validation loss 9472.9445\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9531.5723,validation loss 9422.1980\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9483.2049,validation loss 9319.6393\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9539.1827,validation loss 9335.0087\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9522.2879,validation loss 9313.8618\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9497.1303,validation loss 9329.0332\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9506.1283,validation loss 9328.6562\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9482.3315,validation loss 9340.5354\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9511.1684,validation loss 9322.3997\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9498.8278,validation loss 9352.3337\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9482.1689,validation loss 9313.2849\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9494.9332,validation loss 9332.8436\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9490.8031,validation loss 9365.4836\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9500.3822,validation loss 9416.1099\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9478.8078,validation loss 9322.3388\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9537.4142,validation loss 9338.8988\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9505.9055,validation loss 9377.8530\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9502.9547,validation loss 9304.5431\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9488.4408,validation loss 9303.8833\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9489.0471,validation loss 9325.8036\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9481.0697,validation loss 9341.2544\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9498.8156,validation loss 9333.0138\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9487.8014,validation loss 9301.2579\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9489.9846,validation loss 9293.9354\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9494.0034,validation loss 9288.4423\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9476.7084,validation loss 9372.5846\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9491.2963,validation loss 9298.6889\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9498.0067,validation loss 9314.7293\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9502.8169,validation loss 9286.2657\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9488.8515,validation loss 9342.7878\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9517.1979,validation loss 9289.6203\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9480.4556,validation loss 9329.8521\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9474.0476,validation loss 9337.5078\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9511.3025,validation loss 9372.1490\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9520.7749,validation loss 9291.8989\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9485.2520,validation loss 9319.2008\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13229.3849,validation loss 11764.6493\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 12013.4565,validation loss 11289.1946\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11377.4814,validation loss 10767.9861\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 10969.3736,validation loss 10314.1771\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10645.5198,validation loss 10139.3018\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10461.0251,validation loss 9973.7915\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10308.7270,validation loss 9822.9932\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10175.5904,validation loss 9703.8716\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10057.6275,validation loss 9598.6659\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 10014.3080,validation loss 9511.1925\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9884.8398,validation loss 9407.4160\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9786.6951,validation loss 9237.8585\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9642.0239,validation loss 9157.2559\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9526.7562,validation loss 9083.2991\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9478.8135,validation loss 8944.3138\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9331.7188,validation loss 8876.7981\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9297.2967,validation loss 8863.4497\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9196.2198,validation loss 8798.2679\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9191.8187,validation loss 8722.3115\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9132.4690,validation loss 8690.5029\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9099.4558,validation loss 8649.8027\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9063.5933,validation loss 8613.1517\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 8982.4377,validation loss 8550.5064\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 8971.9104,validation loss 8534.7256\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 8965.5264,validation loss 8544.3247\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 8905.5033,validation loss 8544.6380\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 8876.7258,validation loss 8613.3475\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 8880.4139,validation loss 8614.2261\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 8874.9706,validation loss 8504.8668\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 8893.8973,validation loss 8653.2735\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 8884.7618,validation loss 8492.0297\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 8876.1542,validation loss 8517.9855\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 8891.8682,validation loss 8527.7041\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 8883.4283,validation loss 8595.8602\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 8900.2473,validation loss 8524.5789\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 8882.1256,validation loss 8503.9425\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 8920.7964,validation loss 8515.6751\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 8888.4926,validation loss 8538.1339\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 8912.4642,validation loss 8509.5788\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 8874.8562,validation loss 8493.7107\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 8888.5550,validation loss 8530.4236\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 8903.5343,validation loss 8494.8471\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 8860.6637,validation loss 8540.9690\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 8873.2083,validation loss 8500.0343\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 8883.0754,validation loss 8523.9305\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 8877.8548,validation loss 8562.5477\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 8863.9714,validation loss 8611.7708\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 8907.0498,validation loss 8532.5140\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 8886.5447,validation loss 8610.7123\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 8892.1887,validation loss 8549.1820\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13221.3451,validation loss 11664.1157\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11914.0821,validation loss 11021.9801\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11511.4900,validation loss 10820.8769\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11253.1946,validation loss 10624.5531\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 11112.2867,validation loss 10459.7702\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10998.5179,validation loss 10383.9512\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10804.0388,validation loss 10138.4385\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10441.7132,validation loss 9798.1259\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10240.6855,validation loss 9618.6136\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 10103.9475,validation loss 9498.2533\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9953.3325,validation loss 9421.3430\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9857.7630,validation loss 9251.1746\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9735.7979,validation loss 9276.9500\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9647.2923,validation loss 9133.5673\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9638.6417,validation loss 9150.7097\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9568.5022,validation loss 9132.2932\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9578.9519,validation loss 9092.4982\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9586.3334,validation loss 9120.1408\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9581.8389,validation loss 9189.9029\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9592.7828,validation loss 9165.4638\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9575.6675,validation loss 9157.6347\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9568.5010,validation loss 9128.0256\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9591.7937,validation loss 9068.3774\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9576.2981,validation loss 9092.3895\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9588.7502,validation loss 9115.4770\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9586.8796,validation loss 9118.9493\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9544.2883,validation loss 9076.8966\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9541.4700,validation loss 9066.5734\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9593.6873,validation loss 9063.9233\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9581.9543,validation loss 9089.9311\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9573.5018,validation loss 9098.3159\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9557.3688,validation loss 9119.9502\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9573.7928,validation loss 9116.2177\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9587.0710,validation loss 9121.1953\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9589.7406,validation loss 9091.0658\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9583.2588,validation loss 9177.6410\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9568.7171,validation loss 9128.3348\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9576.7362,validation loss 9094.7573\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9598.7437,validation loss 9202.4987\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9623.1069,validation loss 9151.8861\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9575.3766,validation loss 9154.2982\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9573.9305,validation loss 9227.2780\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9568.2201,validation loss 9078.4775\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9559.9940,validation loss 9081.7580\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9576.5099,validation loss 9119.1220\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9570.8995,validation loss 9087.6219\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9601.6728,validation loss 9087.2802\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9573.5665,validation loss 9221.8384\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9570.3467,validation loss 9089.5179\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9588.1980,validation loss 9185.5316\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13112.2485,validation loss 11895.7565\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 12055.9907,validation loss 11277.3984\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11499.5557,validation loss 10864.4864\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11173.3394,validation loss 10445.4847\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10756.7607,validation loss 10194.9567\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10570.7301,validation loss 10037.6590\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10438.5828,validation loss 9922.8148\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10243.7054,validation loss 9838.8121\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10153.9776,validation loss 9732.2352\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 10051.4118,validation loss 9608.9278\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9972.6005,validation loss 9624.4859\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9900.6748,validation loss 9544.6455\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9900.5217,validation loss 9542.0897\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9861.8385,validation loss 9524.1905\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9860.0624,validation loss 9556.5973\n",
      "-------- Epoch 015 --------\n",
      "Epoch 016: training loss 9880.1349,validation loss 9459.1486\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9846.7529,validation loss 9521.5849\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9861.3191,validation loss 9488.7808\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9858.0297,validation loss 9538.1017\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9844.2825,validation loss 9491.1628\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9824.0037,validation loss 9461.0403\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9833.8671,validation loss 9512.5650\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9828.3912,validation loss 9481.1205\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9882.8475,validation loss 9531.0880\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9836.3982,validation loss 9485.1483\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9815.0310,validation loss 9519.3829\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9862.1251,validation loss 9499.3973\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9850.2672,validation loss 9515.1410\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9829.0468,validation loss 9501.8924\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9842.3890,validation loss 9458.8914\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9871.0353,validation loss 9455.3760\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9854.1785,validation loss 9535.1823\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9871.0208,validation loss 9553.4577\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9840.3622,validation loss 9497.3649\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9821.2740,validation loss 9519.5882\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9855.9475,validation loss 9503.6471\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9856.1054,validation loss 9569.7216\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9857.9522,validation loss 9539.5277\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9815.1379,validation loss 9470.3603\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9825.2614,validation loss 9597.6543\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9844.8428,validation loss 9451.4714\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9835.4405,validation loss 9497.6170\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9821.4299,validation loss 9506.5797\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9813.9415,validation loss 9503.9972\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9832.9694,validation loss 9506.8804\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9830.4226,validation loss 9516.3233\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9848.7450,validation loss 9527.3727\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9858.4137,validation loss 9529.0233\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9819.8395,validation loss 9575.4825\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 12568.2013,validation loss 443083.9446\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11272.6835,validation loss 14816.5917\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 10776.2210,validation loss 12171.0235\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 10352.0060,validation loss 11760.4163\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10086.1745,validation loss 11502.9264\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 9882.3737,validation loss 11414.6074\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 9743.7603,validation loss 11231.4900\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 9614.7048,validation loss 11112.3480\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 9507.8499,validation loss 10963.5069\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9443.5671,validation loss 10909.3692\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9355.7445,validation loss 10836.2224\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9267.6004,validation loss 10785.8395\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9176.2773,validation loss 10625.1596\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9067.9606,validation loss 10471.4613\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 8918.2215,validation loss 10354.8618\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 8845.7998,validation loss 10222.5778\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 8781.0023,validation loss 10135.3717\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 8717.2502,validation loss 10058.4820\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 8683.3631,validation loss 10007.8614\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 8666.3204,validation loss 9984.5462\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 8618.9458,validation loss 9949.4174\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 8597.7538,validation loss 9881.6276\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 8544.6142,validation loss 9844.8899\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 8528.2534,validation loss 9807.6491\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 8505.1835,validation loss 9789.4900\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 8488.5506,validation loss 9765.1125\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 8468.3159,validation loss 9730.3169\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 8445.0055,validation loss 9733.1158\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 8397.7345,validation loss 9736.6316\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 8385.1671,validation loss 9730.0991\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 8351.3651,validation loss 9728.2894\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 8403.7215,validation loss 9659.7356\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 8386.8747,validation loss 9715.5589\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 8362.6637,validation loss 9740.5346\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 8382.3347,validation loss 9687.1355\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 8352.2703,validation loss 9693.2801\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 8391.9884,validation loss 9757.6333\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 8385.9210,validation loss 9680.0282\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 8386.9193,validation loss 9708.7522\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 8389.0083,validation loss 9729.6187\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 8357.1224,validation loss 9684.0878\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 8387.3520,validation loss 9716.0609\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 8368.0534,validation loss 9678.4412\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 8395.5962,validation loss 9701.4956\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 8355.2762,validation loss 9707.8762\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 8357.3434,validation loss 9666.9516\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 8370.7890,validation loss 9733.2145\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 8393.1194,validation loss 9673.1773\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 8376.6414,validation loss 9728.7224\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 8342.7941,validation loss 9704.1775\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 12769.8038,validation loss 13983854.5920\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11365.3425,validation loss 12219.0027\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 10789.8590,validation loss 11631.1970\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 10449.5867,validation loss 11348.8217\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10269.8030,validation loss 11118.1077\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10045.1715,validation loss 11059.6459\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 9947.0214,validation loss 10818.1225\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 9800.1356,validation loss 10739.8820\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 9721.7841,validation loss 10680.8905\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9611.2945,validation loss 10577.0491\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9500.5065,validation loss 10505.1559\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9456.9103,validation loss 10351.7414\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9243.6527,validation loss 10092.5682\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9111.6927,validation loss 10013.0501\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9032.3192,validation loss 9898.8085\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9015.5914,validation loss 9849.3303\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 8917.1774,validation loss 9741.2312\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 8871.7005,validation loss 9689.7051\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 8845.0648,validation loss 9663.1026\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 8798.3872,validation loss 9592.0640\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 8750.9063,validation loss 9563.5346\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 8683.9796,validation loss 9484.4368\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 8692.3317,validation loss 9457.3889\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 8637.0079,validation loss 9490.8927\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 8638.5190,validation loss 9442.1635\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 8593.0789,validation loss 9412.7185\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 8584.3587,validation loss 9386.6223\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 8597.1103,validation loss 9458.1276\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 8571.3902,validation loss 9453.7604\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 8593.6756,validation loss 9374.0249\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 8569.3018,validation loss 9430.7576\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 8565.7205,validation loss 9411.4107\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 8574.8180,validation loss 9411.0109\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 8582.9013,validation loss 9382.1723\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 8586.3828,validation loss 9461.2002\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 8564.8767,validation loss 9385.9586\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 8577.3303,validation loss 9499.9275\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 8606.7003,validation loss 9412.3236\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 8600.9101,validation loss 9473.8433\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 8619.6691,validation loss 9397.6512\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 8581.4943,validation loss 9447.8023\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 8596.1930,validation loss 9433.6767\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 8569.0131,validation loss 9442.0431\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 8576.8200,validation loss 9459.9721\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 8573.9880,validation loss 9436.6214\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 8593.0960,validation loss 9402.3728\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 8551.2550,validation loss 9433.2625\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 8585.4457,validation loss 9394.1984\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 8576.9064,validation loss 9431.2942\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 8576.5064,validation loss 9388.4456\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13378.4943,validation loss 11163.6107\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11999.5541,validation loss 10647.3230\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11506.3317,validation loss 10228.6100\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11037.8965,validation loss 9960.7251\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10810.9904,validation loss 9833.2192\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10580.2277,validation loss 9652.2398\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10430.9672,validation loss 9430.9714\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10325.7462,validation loss 9382.2181\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10223.9445,validation loss 9245.3748\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 10121.1492,validation loss 9138.2411\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 10070.5744,validation loss 9098.8618\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 10038.5053,validation loss 9018.5802\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9950.8589,validation loss 9032.2395\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9861.0501,validation loss 8937.0175\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9839.2789,validation loss 8876.2731\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9836.0957,validation loss 8933.0894\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9824.8303,validation loss 8940.7472\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9786.5380,validation loss 8906.1796\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9762.1712,validation loss 8932.8761\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9779.2374,validation loss 8923.1041\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9777.7344,validation loss 8929.6893\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9823.9206,validation loss 8909.2957\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9805.8283,validation loss 8938.8547\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9791.2779,validation loss 9006.6504\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9822.6571,validation loss 8917.2974\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9763.0726,validation loss 8920.1273\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9803.4620,validation loss 8895.2742\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9800.5759,validation loss 8910.1004\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9793.2407,validation loss 8935.0133\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9827.4024,validation loss 8937.0510\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9794.0844,validation loss 8870.5751\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9802.1221,validation loss 9022.5567\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9815.0804,validation loss 8963.7673\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9796.5715,validation loss 8932.0114\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9783.7358,validation loss 8905.1396\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9809.3616,validation loss 8915.8528\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9789.3622,validation loss 8920.3456\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9814.5089,validation loss 8933.7736\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9810.1370,validation loss 8928.0646\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9788.5342,validation loss 8939.4431\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9844.3115,validation loss 8900.8333\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9820.9063,validation loss 8913.5986\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9784.1527,validation loss 8949.7720\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9801.1273,validation loss 8907.3562\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9833.6678,validation loss 8908.6921\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9794.3185,validation loss 8900.2451\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9829.8988,validation loss 8899.0260\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9816.5799,validation loss 8915.6524\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9813.7796,validation loss 8912.9799\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9777.2432,validation loss 8913.8547\n"
     ]
    }
   ],
   "source": [
    "unconditional_rand_scores = []\n",
    "unconditional_test_mse = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    X_train, X_test = train_test_split(\n",
    "        X,\n",
    "        test_size=0.25,\n",
    "        shuffle=True,\n",
    "        random_state=i)\n",
    "    \n",
    "    \n",
    "    ADVAE = pmVAEModel(\n",
    "        membership_mask.values,\n",
    "        [12],\n",
    "        4,\n",
    "        beta=1e-05,\n",
    "        terms=membership_mask.index,\n",
    "        add_auxiliary_module=True)\n",
    "    \n",
    "    \n",
    "    train_ds = RNASeqData(X_train.values)\n",
    "    test_ds = RNASeqData(X_test.values)\n",
    "    \n",
    "    ADVAE.train(train_ds, test_ds,batch_size=32,\n",
    "                max_epochs=50,\n",
    "                checkpoint_path='pmvae_no_c_4d_repeat_'+ str(i)+ '_030222.pkl')\n",
    "    \n",
    "    ADVAE.load_checkpoint('pmvae_no_c_4d_repeat_' + str(i) + '_030222.pkl.best_loss')\n",
    "    \n",
    "    outputs = ADVAE.model(torch.tensor(X.values).cuda().float())\n",
    "\n",
    "    latent_space = pd.DataFrame(\n",
    "        outputs.z.detach().cpu().numpy(),\n",
    "        index=X.index,\n",
    "        columns=ADVAE.latent_space_names())\n",
    "\n",
    "    \n",
    "\n",
    "    tsne = TSNE(n_components=2,perplexity=40.0)\n",
    "    tsne = pd.DataFrame(\n",
    "        TSNE().fit_transform(latent_space.values),\n",
    "        index=latent_space.index,\n",
    "        columns=['tsne 1', 'tsne 2'])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=6, random_state=0).fit(tsne)\n",
    "    \n",
    "    unconditional_rand_scores.append(adjusted_rand_score(dataset_labels['Dataset Label'].values[~microarray],kmeans.labels_))\n",
    "    \n",
    "    test_outputs = ADVAE.model(torch.tensor(X_test.values).cuda().float())\n",
    "    \n",
    "    unconditional_test_mse.append(F.mse_loss(test_outputs.global_recon, torch.tensor(X_test.values).cuda().float()).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train models with dataset / region conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "c_array = np.concatenate((region_labels.Region.values.reshape(-1,1),\n",
    "                          dataset_labels['Dataset Label'].values.reshape(-1,1)),axis=1)\n",
    "\n",
    "enc.fit(c_array)\n",
    "\n",
    "pre_processed_c_array = enc.transform(c_array).toarray()\n",
    "\n",
    "microarray = dataset_labels['Dataset Label'] == \"MSBB_MICROARRAY\"\n",
    "\n",
    "X = rna_seq.loc[~microarray.values,:]\n",
    "pre_processed_c_array = pre_processed_c_array[~microarray,:]\n",
    "\n",
    "assignment_mat = load_annotations(\n",
    "    'data/h.all.v7.4.symbols.gmt',\n",
    "    X.columns,\n",
    "    min_genes=20\n",
    ")\n",
    "\n",
    "membership_mask = assignment_mat.astype(bool).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13729.6083,validation loss 11698.0188\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 12111.5148,validation loss 10631.5992\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11443.3245,validation loss 10230.3455\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 10936.2839,validation loss 9899.6742\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10679.2139,validation loss 9613.4061\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10436.9265,validation loss 9381.8372\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10214.3550,validation loss 9314.8218\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10127.0730,validation loss 9178.9609\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 9986.5681,validation loss 9033.2419\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9890.6555,validation loss 9043.1924\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9799.7582,validation loss 8970.7740\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9738.0968,validation loss 8957.1611\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9709.2011,validation loss 8882.1910\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9689.0279,validation loss 8840.1476\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9721.7380,validation loss 8850.8913\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9668.6570,validation loss 8945.3389\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9686.2064,validation loss 8929.4993\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9685.9862,validation loss 8804.7267\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9684.6836,validation loss 8844.9987\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9674.7192,validation loss 8819.4668\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9645.8767,validation loss 8841.4025\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9685.8342,validation loss 8837.6391\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9676.2937,validation loss 8890.5934\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9697.9223,validation loss 8851.1680\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9668.0559,validation loss 8909.7102\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9696.8841,validation loss 8903.4820\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9677.9577,validation loss 8834.1133\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9674.6597,validation loss 8849.0955\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9681.1493,validation loss 8902.8440\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9668.2889,validation loss 8864.8722\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9686.4912,validation loss 8936.8977\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9695.1535,validation loss 8911.5567\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9657.1600,validation loss 8867.1684\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9663.8556,validation loss 8841.5989\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9686.8262,validation loss 8838.5125\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9681.9780,validation loss 8889.1804\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9689.9942,validation loss 8851.8711\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9655.6182,validation loss 8831.0909\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9666.8412,validation loss 8890.9229\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9680.4523,validation loss 8810.9002\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9688.4456,validation loss 8865.6424\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9694.8855,validation loss 8820.2921\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9679.6863,validation loss 8989.6272\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9668.9333,validation loss 8854.6902\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9670.6886,validation loss 8885.3105\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9674.7301,validation loss 8932.1673\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9694.6686,validation loss 8868.4587\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9670.8604,validation loss 8855.0587\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9690.3157,validation loss 8839.5023\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9669.5642,validation loss 8889.1673\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13887.1663,validation loss 11379.5307\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 12388.1065,validation loss 10651.9781\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11679.2932,validation loss 9961.4148\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11250.1136,validation loss 9730.8758\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10922.2161,validation loss 9272.0052\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10622.1232,validation loss 9051.4037\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10386.9927,validation loss 8840.4319\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10252.6353,validation loss 8706.7080\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10139.1968,validation loss 8667.0952\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 10029.2567,validation loss 8527.5633\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9956.7029,validation loss 8496.9037\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9893.6091,validation loss 8398.3618\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9787.2107,validation loss 8344.0750\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9705.6426,validation loss 8177.5900\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9612.6197,validation loss 8056.8364\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9474.3348,validation loss 7976.7607\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9392.2714,validation loss 7909.8076\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9313.4435,validation loss 7874.8490\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9284.1102,validation loss 7924.4474\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9264.8667,validation loss 7797.0412\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9191.8916,validation loss 7769.5245\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9227.9347,validation loss 7861.6113\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9198.6947,validation loss 7768.8226\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9156.6357,validation loss 7757.0791\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9191.6613,validation loss 7738.9307\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9162.5563,validation loss 7775.5311\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9200.6732,validation loss 7765.2040\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9199.3958,validation loss 7754.1581\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9185.7532,validation loss 7727.7556\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9186.3621,validation loss 7786.1911\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9177.1868,validation loss 7760.5869\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9189.0889,validation loss 7762.5113\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9200.8267,validation loss 7762.9093\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9160.2077,validation loss 7735.1883\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9178.6228,validation loss 7776.7083\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9170.9131,validation loss 7761.9999\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9205.8798,validation loss 7738.9531\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9197.2113,validation loss 7745.1179\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9195.4557,validation loss 7754.5920\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9161.4958,validation loss 7780.4636\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9190.5039,validation loss 7744.6655\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9191.8169,validation loss 7770.2043\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9186.3585,validation loss 7815.7787\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9196.0381,validation loss 7749.2889\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9201.7726,validation loss 7777.4598\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9201.6238,validation loss 7754.9642\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9162.0266,validation loss 7761.1689\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9185.7005,validation loss 7805.1878\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9170.2229,validation loss 7748.4225\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9152.1926,validation loss 7843.5374\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13304.5149,validation loss 152296.7194\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11809.5135,validation loss 11591.5486\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11090.1691,validation loss 11163.0797\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 10733.1284,validation loss 10859.9335\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10411.3758,validation loss 10529.2384\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10180.1519,validation loss 10240.8481\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 9936.2391,validation loss 9991.2143\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 9791.3672,validation loss 9802.8167\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 9710.5130,validation loss 9766.4715\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9595.6721,validation loss 9620.4883\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9490.2806,validation loss 9593.1382\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9438.5773,validation loss 9491.6247\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9335.3009,validation loss 9397.9636\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9213.7684,validation loss 9279.3333\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9092.7819,validation loss 9179.7332\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 8989.0287,validation loss 9055.7680\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 8919.1497,validation loss 8975.1368\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 8863.4977,validation loss 8925.4743\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 8806.9678,validation loss 8863.2359\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 8814.4550,validation loss 8855.6590\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 8743.7397,validation loss 8837.7175\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 8709.2951,validation loss 8768.2757\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 8666.9340,validation loss 8742.0616\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 8631.3816,validation loss 8704.9869\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 8608.9054,validation loss 8659.3869\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 8576.5479,validation loss 8617.4913\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 8569.5443,validation loss 8618.5257\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 8498.9109,validation loss 8589.4863\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 8465.9686,validation loss 8577.5997\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 8489.8139,validation loss 8621.3548\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 8467.9811,validation loss 8549.2836\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 8484.6037,validation loss 8532.5008\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 8459.1196,validation loss 8558.2062\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 8475.5593,validation loss 8572.5917\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 8483.3138,validation loss 8565.9142\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 8452.5031,validation loss 8551.3577\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 8496.7737,validation loss 8574.9982\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 8464.0408,validation loss 8569.5839\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 8511.9734,validation loss 8564.2692\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 8491.5623,validation loss 8558.8210\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 8473.2024,validation loss 8543.4240\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 8492.9139,validation loss 8567.5910\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 8491.0116,validation loss 8550.6483\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 8505.7542,validation loss 8537.6847\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 8463.9488,validation loss 8562.1529\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 8456.6231,validation loss 8608.4932\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 8484.6797,validation loss 8569.2830\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 8501.3198,validation loss 8574.9179\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 8453.5319,validation loss 8563.8741\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 8475.3746,validation loss 8552.7332\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13660.3584,validation loss 11764.6950\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 12180.5084,validation loss 11013.6600\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11577.2001,validation loss 10502.2842\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11094.3614,validation loss 10158.2329\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10839.7642,validation loss 10018.2148\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10589.5465,validation loss 9692.5179\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10356.5200,validation loss 9449.6022\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10179.1929,validation loss 9317.4520\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10001.8956,validation loss 9177.6734\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9895.7147,validation loss 9104.6872\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9816.2901,validation loss 8956.9738\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9724.6473,validation loss 8969.3111\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9661.1980,validation loss 8901.9475\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9635.3927,validation loss 8853.4608\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9622.7013,validation loss 8856.7056\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9593.0888,validation loss 8863.8663\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9599.7155,validation loss 8839.6894\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9574.0768,validation loss 8919.1013\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9575.9389,validation loss 8841.4101\n",
      "-------- Epoch 019 --------\n",
      "Epoch 029: training loss 9573.2980,validation loss 8871.7808\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9595.2110,validation loss 8811.3630\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9573.0777,validation loss 8877.4287\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9593.5560,validation loss 8876.7007\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9578.3342,validation loss 8835.1894\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9609.8302,validation loss 8827.0791\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9586.6954,validation loss 8978.1867\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9600.3151,validation loss 8917.2112\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9602.5122,validation loss 8937.2734\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9591.0297,validation loss 8798.6507\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9578.8540,validation loss 8831.3139\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9615.8876,validation loss 8836.7834\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9597.2959,validation loss 8868.0589\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9565.7942,validation loss 8810.5915\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9590.2221,validation loss 8861.2343\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9566.6774,validation loss 8844.0047\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9572.4688,validation loss 8846.7148\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9585.2552,validation loss 8829.3440\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9576.0515,validation loss 8877.0476\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9579.7119,validation loss 8811.8786\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9610.3550,validation loss 8916.1474\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13719.5966,validation loss 11529.3384\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 12168.3152,validation loss 10816.0638\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11487.3324,validation loss 10322.6652\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11146.9653,validation loss 9968.0691\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10857.1577,validation loss 9677.4254\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10523.4874,validation loss 9433.0919\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10329.3009,validation loss 9335.7077\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10227.0229,validation loss 9190.3852\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10071.7788,validation loss 8994.4910\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9937.8868,validation loss 8974.0317\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9784.9334,validation loss 8776.9144\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9661.1916,validation loss 8679.1220\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9535.1236,validation loss 8478.0717\n",
      "-------- Epoch 013 --------\n",
      "Epoch 019: training loss 9066.2924,validation loss 8100.0013\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9030.7315,validation loss 8103.9237\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9028.7242,validation loss 8107.6956\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9042.1407,validation loss 8111.7028\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9028.7830,validation loss 8067.2547\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9066.4360,validation loss 8073.7321\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9023.5230,validation loss 8110.9438\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9027.0426,validation loss 8078.9249\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9027.2150,validation loss 8134.9960\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9035.4374,validation loss 8144.3599\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9018.9373,validation loss 8087.1333\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9033.7023,validation loss 8196.5731\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9015.5881,validation loss 8065.2791\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 8999.5027,validation loss 8075.9154\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 8995.3433,validation loss 8186.4610\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9025.9066,validation loss 8285.5391\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9004.2780,validation loss 8104.3262\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9016.6966,validation loss 8119.7878\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9050.2734,validation loss 8231.8366\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9005.5671,validation loss 8084.4114\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9003.9402,validation loss 8076.1355\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9043.9063,validation loss 8124.1891\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9018.5520,validation loss 8185.6585\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 8980.6101,validation loss 8104.4651\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9051.6426,validation loss 8065.4749\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9027.9473,validation loss 8133.8586\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9045.5469,validation loss 8126.7502\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9010.0557,validation loss 8083.7538\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9032.7946,validation loss 8083.2608\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9024.8131,validation loss 8127.4403\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9013.8512,validation loss 8098.1883\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13794.6420,validation loss 11493.3315\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 12179.9617,validation loss 10673.5880\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11517.8275,validation loss 10233.4432\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11105.6122,validation loss 9912.7402\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10806.7916,validation loss 9663.0077\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10538.8050,validation loss 9497.4207\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10353.7431,validation loss 9147.2703\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10169.5553,validation loss 9002.4949\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10014.2360,validation loss 8899.7503\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9931.4842,validation loss 8819.0762\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9829.0641,validation loss 8747.8140\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9730.9094,validation loss 8865.5933\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9674.5210,validation loss 8677.3023\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9660.9598,validation loss 8714.8519\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9677.2175,validation loss 8808.9336\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9626.8649,validation loss 8623.9647\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9650.8858,validation loss 8695.0078\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9595.0599,validation loss 8643.6561\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9624.4199,validation loss 8640.9765\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9663.7093,validation loss 8632.5038\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9660.9579,validation loss 8683.7000\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9686.1869,validation loss 8670.2050\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9637.1727,validation loss 8681.0916\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9673.2511,validation loss 8657.0071\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9662.7235,validation loss 8641.0992\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9656.0933,validation loss 8640.2056\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9638.6596,validation loss 8649.7690\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9622.9050,validation loss 8603.3892\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9633.1904,validation loss 8622.2384\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9644.9546,validation loss 8623.0837\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9628.1884,validation loss 8664.2222\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9641.7305,validation loss 8673.7023\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9648.7478,validation loss 8668.0013\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9640.8787,validation loss 8613.5025\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9630.4518,validation loss 8695.8996\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9649.2851,validation loss 8685.8591\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9655.6325,validation loss 8740.5610\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9637.9162,validation loss 8644.6597\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9662.3626,validation loss 8641.9718\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9667.6015,validation loss 8622.0985\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9674.9232,validation loss 8791.4596\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9625.6936,validation loss 8676.9801\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9626.6924,validation loss 8651.2597\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9646.2615,validation loss 8642.6414\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9643.1317,validation loss 8700.3464\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9622.8233,validation loss 8653.3077\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9671.4142,validation loss 8675.3202\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9667.5340,validation loss 8622.3330\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9681.4534,validation loss 8637.0635\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9635.9572,validation loss 8713.2734\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13681.6174,validation loss 11549.5450\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 12233.0709,validation loss 10904.2281\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11657.7594,validation loss 10388.3466\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11103.7952,validation loss 9988.6846\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10873.0545,validation loss 9780.5441\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10620.3552,validation loss 9524.7647\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10353.1565,validation loss 9232.5907\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10130.7264,validation loss 9062.8091\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10043.7973,validation loss 8982.1924\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9917.3843,validation loss 8859.5201\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9772.2671,validation loss 8755.0969\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9710.8607,validation loss 8638.8716\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9547.7017,validation loss 8514.3968\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9472.2069,validation loss 8389.0658\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9384.7327,validation loss 8316.6664\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9266.0856,validation loss 8195.6492\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9232.4513,validation loss 8173.5554\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9213.3046,validation loss 8433.8124\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9090.4741,validation loss 8120.3777\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9080.3353,validation loss 8094.5761\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9079.8444,validation loss 8119.9834\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9048.8506,validation loss 8081.7060\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9075.6090,validation loss 8056.5015\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9089.8732,validation loss 8103.0173\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9042.7033,validation loss 8163.0635\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9060.5412,validation loss 8074.4751\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9050.0623,validation loss 8245.3098\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9061.9930,validation loss 8070.2368\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9049.7225,validation loss 8079.9367\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9072.0446,validation loss 8053.0237\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9066.6179,validation loss 8070.5487\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9081.3060,validation loss 8051.3757\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9083.3893,validation loss 8056.1563\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9052.9686,validation loss 8087.7785\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9041.2903,validation loss 8177.3222\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9075.5831,validation loss 8123.0596\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9047.8020,validation loss 8086.0643\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9054.4797,validation loss 8094.2867\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9066.8249,validation loss 8090.1991\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9020.3673,validation loss 8077.1441\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9070.0224,validation loss 8063.2873\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9076.5830,validation loss 8077.5223\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9081.3383,validation loss 8076.8584\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9029.2356,validation loss 8228.8718\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9036.4240,validation loss 8076.0503\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9071.7174,validation loss 8072.8551\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9042.7554,validation loss 8101.5449\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9053.2990,validation loss 8080.0739\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9035.5546,validation loss 8082.7538\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9080.8773,validation loss 8063.8006\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13114.1056,validation loss 18004.5528\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11625.9447,validation loss 25952.7815\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11220.8149,validation loss 23866.4734\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11119.9866,validation loss 17915.1567\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 11131.2564,validation loss 12854.9174\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 11162.0622,validation loss 74585.6254\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 11115.2169,validation loss 14331.9388\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 11168.9693,validation loss 15757.4851\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 11151.2388,validation loss 13531.2080\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 11112.3199,validation loss 18878.6085\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 11132.4653,validation loss 12629.1172\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 11161.2147,validation loss 13152.9577\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 11118.4144,validation loss 12408.9954\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 11128.9867,validation loss 14045.6242\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 11141.2855,validation loss 15192.8522\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 11127.4308,validation loss 13126.1656\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 11153.7619,validation loss 12420.0535\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 11131.6396,validation loss 58336.5732\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 11144.6458,validation loss 18774.1963\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 11134.0581,validation loss 42124.4372\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 11120.7916,validation loss 19000.9896\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 11138.6086,validation loss 16712.0003\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 11138.9910,validation loss 15114.7291\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 11104.7799,validation loss 12769.5036\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 11142.6362,validation loss 14618.7250\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 11139.6913,validation loss 12535.0526\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 11132.4475,validation loss 15773.6058\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 11104.9336,validation loss 13663.1539\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 11157.6381,validation loss 13153.6421\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 11127.2365,validation loss 13610.6492\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 11114.0497,validation loss 12388.0277\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 11105.9655,validation loss 13592.8433\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 11095.1965,validation loss 16099.1809\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 11115.3632,validation loss 48972.5514\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 11134.6883,validation loss 14967.2846\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 11100.7397,validation loss 36405.1986\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 11119.7950,validation loss 15139.5719\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 11157.4839,validation loss 12673.8045\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 11128.1359,validation loss 60452.3680\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 11153.6108,validation loss 23412.1462\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 11110.4007,validation loss 14146.1255\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 11125.5465,validation loss 13395.8242\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 11125.6811,validation loss 12970.2885\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 11111.4034,validation loss 15182.8301\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 11127.8442,validation loss 16062.8050\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 11109.2744,validation loss 12392.6200\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 11112.3938,validation loss 15000.7783\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 11112.3536,validation loss 12638.8252\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 11127.5922,validation loss 18206.9492\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 11132.7856,validation loss 15298.3480\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13353.2669,validation loss 96521.3169\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 11956.4259,validation loss 12906.9446\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11187.7230,validation loss 11386.8269\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 10707.1361,validation loss 10968.3889\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10330.6390,validation loss 10731.5986\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10068.4005,validation loss 10429.6259\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 9866.7767,validation loss 10198.7887\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 9741.8925,validation loss 10100.7752\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 9588.5139,validation loss 9974.9838\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 9480.9423,validation loss 9830.5150\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 9396.8357,validation loss 9776.8735\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9342.7407,validation loss 9636.0656\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9268.4706,validation loss 9517.2056\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9169.2669,validation loss 9452.1550\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9110.3071,validation loss 9363.1801\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9012.9787,validation loss 9432.4470\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 8933.4181,validation loss 9289.9692\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 8921.2821,validation loss 9315.8413\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 8922.2973,validation loss 9307.4171\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 8925.3186,validation loss 9301.1100\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 8881.8474,validation loss 9281.9163\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 8926.6595,validation loss 9278.2476\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 8900.3484,validation loss 9273.2202\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 8896.6965,validation loss 9275.4811\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 8903.8080,validation loss 9311.5039\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 8924.5129,validation loss 9298.5524\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 8921.1750,validation loss 9343.6428\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 8934.4353,validation loss 9306.9799\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 8906.8051,validation loss 9302.0332\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 8895.5421,validation loss 9283.5121\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 8902.0542,validation loss 9287.5050\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 8937.2588,validation loss 9332.3734\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 8924.6049,validation loss 9267.4464\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 8905.3782,validation loss 9297.3328\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 8894.2535,validation loss 9289.6044\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 8920.6680,validation loss 9298.8735\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 8928.4582,validation loss 9293.9252\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 8937.2890,validation loss 9284.0499\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 8905.0843,validation loss 9319.3365\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 8913.7735,validation loss 9273.7207\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 8898.0800,validation loss 9288.2800\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 8925.5289,validation loss 9304.1132\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 8928.5663,validation loss 9313.8440\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 8929.3207,validation loss 9315.6923\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 8933.3496,validation loss 9288.9863\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 8895.2098,validation loss 9271.5813\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 8898.2957,validation loss 9284.9390\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 8911.0367,validation loss 9307.7751\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 8907.6086,validation loss 9291.4127\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 8906.3187,validation loss 9288.3270\n",
      "-------- Epoch 000 --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/jjanizek/ad_deep_plier/models.py:798: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_weight = torch.tensor(sample_weight, dtype=y_pred.dtype).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: training loss 13965.4455,validation loss 11208.4593\n",
      "-------- Epoch 001 --------\n",
      "Epoch 001: training loss 12458.3583,validation loss 10546.8603\n",
      "-------- Epoch 002 --------\n",
      "Epoch 002: training loss 11729.9578,validation loss 9901.6269\n",
      "-------- Epoch 003 --------\n",
      "Epoch 003: training loss 11334.0696,validation loss 9571.6916\n",
      "-------- Epoch 004 --------\n",
      "Epoch 004: training loss 10932.9029,validation loss 9253.4137\n",
      "-------- Epoch 005 --------\n",
      "Epoch 005: training loss 10648.6382,validation loss 9079.6861\n",
      "-------- Epoch 006 --------\n",
      "Epoch 006: training loss 10468.9744,validation loss 8957.6875\n",
      "-------- Epoch 007 --------\n",
      "Epoch 007: training loss 10361.6254,validation loss 8845.0969\n",
      "-------- Epoch 008 --------\n",
      "Epoch 008: training loss 10216.4468,validation loss 8640.7615\n",
      "-------- Epoch 009 --------\n",
      "Epoch 009: training loss 10098.2835,validation loss 8527.7630\n",
      "-------- Epoch 010 --------\n",
      "Epoch 010: training loss 10005.2997,validation loss 8459.9209\n",
      "-------- Epoch 011 --------\n",
      "Epoch 011: training loss 9937.6201,validation loss 8380.4750\n",
      "-------- Epoch 012 --------\n",
      "Epoch 012: training loss 9846.8406,validation loss 8334.9827\n",
      "-------- Epoch 013 --------\n",
      "Epoch 013: training loss 9803.0944,validation loss 8252.9728\n",
      "-------- Epoch 014 --------\n",
      "Epoch 014: training loss 9777.8053,validation loss 8309.7382\n",
      "-------- Epoch 015 --------\n",
      "Epoch 015: training loss 9624.2796,validation loss 8193.8230\n",
      "-------- Epoch 016 --------\n",
      "Epoch 016: training loss 9645.7380,validation loss 8181.6828\n",
      "-------- Epoch 017 --------\n",
      "Epoch 017: training loss 9631.0862,validation loss 8153.7790\n",
      "-------- Epoch 018 --------\n",
      "Epoch 018: training loss 9626.8700,validation loss 8197.6625\n",
      "-------- Epoch 019 --------\n",
      "Epoch 019: training loss 9564.9787,validation loss 8165.4510\n",
      "-------- Epoch 020 --------\n",
      "Epoch 020: training loss 9606.7593,validation loss 8164.0729\n",
      "-------- Epoch 021 --------\n",
      "Epoch 021: training loss 9584.1766,validation loss 8186.1354\n",
      "-------- Epoch 022 --------\n",
      "Epoch 022: training loss 9647.8670,validation loss 8160.9581\n",
      "-------- Epoch 023 --------\n",
      "Epoch 023: training loss 9586.5780,validation loss 8172.6925\n",
      "-------- Epoch 024 --------\n",
      "Epoch 024: training loss 9604.1939,validation loss 8155.4699\n",
      "-------- Epoch 025 --------\n",
      "Epoch 025: training loss 9657.3616,validation loss 8163.2487\n",
      "-------- Epoch 026 --------\n",
      "Epoch 026: training loss 9592.3451,validation loss 8148.1866\n",
      "-------- Epoch 027 --------\n",
      "Epoch 027: training loss 9570.4603,validation loss 8248.7737\n",
      "-------- Epoch 028 --------\n",
      "Epoch 028: training loss 9586.3660,validation loss 8178.0605\n",
      "-------- Epoch 029 --------\n",
      "Epoch 029: training loss 9611.0319,validation loss 8148.2239\n",
      "-------- Epoch 030 --------\n",
      "Epoch 030: training loss 9578.7317,validation loss 8154.1334\n",
      "-------- Epoch 031 --------\n",
      "Epoch 031: training loss 9572.9511,validation loss 8161.3987\n",
      "-------- Epoch 032 --------\n",
      "Epoch 032: training loss 9570.4883,validation loss 8180.3381\n",
      "-------- Epoch 033 --------\n",
      "Epoch 033: training loss 9604.7923,validation loss 8150.5803\n",
      "-------- Epoch 034 --------\n",
      "Epoch 034: training loss 9596.4428,validation loss 8157.4307\n",
      "-------- Epoch 035 --------\n",
      "Epoch 035: training loss 9602.9775,validation loss 8147.5562\n",
      "-------- Epoch 036 --------\n",
      "Epoch 036: training loss 9586.3249,validation loss 8220.9901\n",
      "-------- Epoch 037 --------\n",
      "Epoch 037: training loss 9617.2490,validation loss 8174.1664\n",
      "-------- Epoch 038 --------\n",
      "Epoch 038: training loss 9636.8239,validation loss 8152.3148\n",
      "-------- Epoch 039 --------\n",
      "Epoch 039: training loss 9601.7528,validation loss 8179.2131\n",
      "-------- Epoch 040 --------\n",
      "Epoch 040: training loss 9590.0181,validation loss 8162.1625\n",
      "-------- Epoch 041 --------\n",
      "Epoch 041: training loss 9593.1333,validation loss 8139.7000\n",
      "-------- Epoch 042 --------\n",
      "Epoch 042: training loss 9574.2105,validation loss 8177.6810\n",
      "-------- Epoch 043 --------\n",
      "Epoch 043: training loss 9571.7578,validation loss 8204.0331\n",
      "-------- Epoch 044 --------\n",
      "Epoch 044: training loss 9566.2261,validation loss 8217.5737\n",
      "-------- Epoch 045 --------\n",
      "Epoch 045: training loss 9595.6326,validation loss 8154.6273\n",
      "-------- Epoch 046 --------\n",
      "Epoch 046: training loss 9596.4130,validation loss 8159.9673\n",
      "-------- Epoch 047 --------\n",
      "Epoch 047: training loss 9622.3786,validation loss 8148.2830\n",
      "-------- Epoch 048 --------\n",
      "Epoch 048: training loss 9593.2867,validation loss 8169.8902\n",
      "-------- Epoch 049 --------\n",
      "Epoch 049: training loss 9588.3730,validation loss 8204.1978\n"
     ]
    }
   ],
   "source": [
    "hsic_conditional_rand_scores = []\n",
    "hsic_conditional_test_mse = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    X_train, X_test, c_train, c_test = train_test_split(\n",
    "        X, pre_processed_c_array,\n",
    "        test_size=0.25,\n",
    "        shuffle=True,\n",
    "        random_state=i,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ADVAE = pmVAEModel(\n",
    "        membership_mask.values,\n",
    "        [12],\n",
    "        4,\n",
    "        cdim = c_train.shape[1],\n",
    "        hsic_penalty=1.0e6,#hsic_penalty=1.0e6,\n",
    "        beta=1e-05,\n",
    "        terms=membership_mask.index,\n",
    "        add_auxiliary_module=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train_ds = RNASeqData(X_train.values, c=c_train)\n",
    "    test_ds = RNASeqData(X_test.values, c=c_test)\n",
    "    \n",
    "    ADVAE.train(train_ds, test_ds, batch_size=32,\n",
    "                max_epochs=50,\n",
    "                checkpoint_path='pmvae_HSIC_4d_repeat_'+ str(i)+ '_030222.pkl')\n",
    "    \n",
    "    ADVAE.load_checkpoint('pmvae_HSIC_4d_repeat_' + str(i) + '_030222.pkl.best_loss')\n",
    "    \n",
    "    outputs = ADVAE.model(torch.tensor(X.values).cuda().float(),\n",
    "                          torch.tensor(pre_processed_c_array).cuda().float())\n",
    "\n",
    "    latent_space = pd.DataFrame(\n",
    "        outputs.z.detach().cpu().numpy(),\n",
    "        index=X.index,\n",
    "        columns=ADVAE.latent_space_names())\n",
    "\n",
    "    \n",
    "\n",
    "    tsne = TSNE(n_components=2,perplexity=40.0)\n",
    "    tsne = pd.DataFrame(\n",
    "        TSNE().fit_transform(latent_space.values),\n",
    "        index=latent_space.index,\n",
    "        columns=['tsne 1', 'tsne 2'])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=6, random_state=0).fit(tsne)\n",
    "    \n",
    "    hsic_conditional_rand_scores.append(adjusted_rand_score(dataset_labels['Dataset Label'].values[~microarray],kmeans.labels_))\n",
    "    \n",
    "    test_outputs = ADVAE.model(torch.tensor(X_test.values).cuda().float(),\n",
    "                               torch.tensor(c_test).cuda().float())\n",
    "    \n",
    "    hsic_conditional_test_mse.append(F.mse_loss(test_outputs.global_recon, torch.tensor(X_test.values).cuda().float()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_mse = unconditional_test_mse+hsic_conditional_test_mse\n",
    "all_ari = unconditional_rand_scores+hsic_conditional_rand_scores\n",
    "all_model_type = ['pmVAE']*10 + ['conditional pmVAE']*10\n",
    "results_df = pd.DataFrame()\n",
    "results_df['model'] = all_model_type\n",
    "results_df['test_mse'] = all_test_mse\n",
    "results_df['ari'] = all_ari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASMAAAFvCAYAAAABuq06AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5wU5Z3v8c+XQa5RLjPecBTR9YZuVGAhRmOMBkRXxTVGYeNGzBp3c9Q9msSs0ZxcMCY55mQ1OatZ0WPiy0SJYmIwbiAmLroRo4AXEIwRvIF4wRnud2Z+54+uwWYYoAemp57p+b5fr35N11NVXb/u6fnOU091VykiMDPLW5e8CzAzA4eRmSXCYWRmSXAYmVkSHEZmlgSHkZkloWveBbQlSVMjYnQJi/rzDGb50PZmVFrPqCbvAsxs11RaGJlZB+UwMrMkOIzMLAkOIzNLgsPIzJLgMDKzJDiMzCwJDiMzS4LDyMyS4DAyS8TatWuZMmUKa9euzbuUXDiMzBIx59GpbJ47m2efeSbvUnJRUV+UNeuo6u+8hYEP3ctAYN1zT7DqwAHseejheZfVrtwzMstZw3vvsP7X922Z7rl+DUvu/GGOFeXDYWSWs4YV9ajZVXo2vPdOTtXkx2FklrM9Dj2SDTX7bdXWOPzknKrJj8PILGfq0oWa79zKq3/1YZbsfzDPDh/J4f/w+bzLancewDZLwIcGHEjjpy5m9ksvMXjwYHr16pV3Se3OYWSWiKFDh7Js2TKGDBmSdym5UCVd3lrSrIgYVsKilfOkzTqWTnMObDProBxGZpYEh5GZJcFhZGZJcBiZWRIcRmaWBIeRmSXBYWRmSXAYmVkSHEZmlgSHkZklwWFkZklwGJlZEhxGZpYEh5GZJcFhZGZJKGsYSRot6WVJCyRd28L8myU9n93+Iml50byGonlTylmnmeWvbKedlVQF3AqMBBYDMyVNiYj5TctExNVFy18JHF/0EOsi4rhy1WdmaSlnz2g4sCAiXo2IjcAkYMwOlh8H3LeD+WZWwcoZRgcAi4qmF2dt25A0EBgEPFbU3EPSLEl/knTu9jYi6bJsuVlATRvUbWY5KOfVQVo68fb2ToQ/FpgcEQ1FbQdFxBJJhwCPSZobEQu3ecCIicBEKJyQf3eLNrN8lLNntBg4sGi6FliynWXH0mwXLSKWZD9fBaaz9XiSmVWYcobRTOAwSYMkdaMQONscFZN0BNAPeKqorZ+k7tn9GuBEYH7zdc2scpRtNy0iNku6ApgGVAF3RcQ8SROAWRHRFEzjgEmx9QXcjgJul9RIITC/V3wUzswqjy/iaGbtyRdxNLO0OYzMLAkOIzNLgsPIzJLgMDKzJDiMzCwJDiMzS4LDyMyS4DAysyQ4jMwsCQ4jM0uCw8jMkuAwMrMkOIzMLAkOIzNLgsPIzJLgMDKzJDiMzCwJDiMzS4LDyMyS4DAysyQ4jMwsCQ4jM0uCw8jMkuAwMrMkOIzMLAkOIzNLgsPIzJLgMDKzJDiMzCwJDiMzS4LDyMyS4DAysyQ4jMwsCQ4jM0uCw8jMkuAwMrMkOIzMLAkOIzNLgsPIzJJQ1jCSNFrSy5IWSLq2hfk3S3o+u/1F0vKieRdLeiW7XVzOOs0sf4qI8jywVAX8BRgJLAZmAuMiYv52lr8SOD4iPiepPzALGAYEMBsYGhHLdrLNWRExrITyyvOkzWxntL0Z5ewZDQcWRMSrEbERmASM2cHy44D7svunA49GRH0WQI8Co8tYq5nlrJxhdACwqGh6cda2DUkDgUHAY61d18wqQ9cyPnZL3bHt7R6NBSZHRENr15V0GXBZNlnTqgrNLBnl7BktBg4smq4Flmxn2bF8sIvWqnUjYmJEDMvGit7f9XLNLE/lDKOZwGGSBknqRiFwpjRfSNIRQD/gqaLmacAoSf0k9QNGZW1mVqHKtpsWEZslXUEhRKqAuyJinqQJwKyIaAqmccCkKDqsFxH1km6gEGgAEyKivly1mln+ynZoPw8+tG+WvFwO7ZuZlcxhZGZJcBiZWRIcRmaWBIeRmSXBYWRmSXAYmVkSHEZmlgSHkZklwWFkZklwGJlZEhxGZpYEh5GZJcFhZGZJcBiZWRIcRmaWBIeRmSXBYWRmSXAYmVkSHEZmlgSHkZklwWFkZklwGJlZEhxGZpYEh5GZJcFhZGZJcBiZWRIcRmaWBIeRmSXBYWRmSXAYmVkSHEZmlgSHkZklwWFkZklwGJlZEhxGZpYEh5GZJcFhZGZJcBiZWRIcRmaWBIeRmSWhrGEkabSklyUtkHTtdpa5QNJ8SfMk3VvU3iDp+ew2pZx1mln+upbrgSVVAbcCI4HFwExJUyJiftEyhwFfBU6MiGWS9il6iHURcVy56jOztJSzZzQcWBARr0bERmASMKbZMp8Hbo2IZQAR8V4Z6zGzhJUzjA4AFhVNL87aih0OHC7pSUl/kjS6aF4PSbOy9nO3txFJl2XLzQJq2qx6M2tXZdtNA9RCW7Sw/cOAU4Ba4L8lHRMRy4GDImKJpEOAxyTNjYiF2zxgxERgIkAWSGbWAZWzZ7QYOLBouhZY0sIyv46ITRHxGvAyhXAiIpZkP18FpgPHl7FWM8tZOcNoJnCYpEGSugFjgeZHxR4CPgEgqYbCbturkvpJ6l7UfiIwHzOrWDsNIxVcJOnr2fRBkobvbL2I2AxcAUwDXgLuj4h5kiZIOidbbBpQJ2k+8F/ANRFRBxwFzJL0Qtb+veKjcGZWeRTRfBin2QLSj4FG4NSIOEpSP+B3EfE37VFga0iaFRHDSlh0x0/azMqlpbFkoLQB7BERMUTScwDZ54G6tVlpZmaUNma0KfsAYwBI2ptCT8nMrM2UEkY/An4F7CPpRuCPwHfKWpWZdTo7HTMCkHQkcBqF/b0/RMRL5S5sV3jMyCx52x0zKuVo2qHAaxFxK/AiMFJS3zYszsyspN20B4EGSX8F3AkMAu7d8SpmZq1TShg1Zp8ZOg/4YURcDexf3rLMrLMp9WjaOOCzwG+ytj3KV5KZdUalhNElwAnAjRHxmqRBwM/KW5aZdTYlHU3rKHw0zSx5u3U07SxJz0mql7RS0ipJK9u2PjPr7Er5OsgtFAav50YldaPMLCmljBktAl50EJlZOZXSM/oK8J+SHgc2NDVGxL+VrSoz63RKCaMbgdVAD8Df1jezsigljPpHxKiyV2JmnVopY0a/l+QwMrOyKuVMj6uA3hTGizZR+JxARMRe5S+vdfw5I7Pk7fqZHiNizx0+snR0RMzblarMzJq0xdVB7mmDxzCzTq4twmi73S4zs1K1RRh5/MXMdls5L+JoZlaytgijjW3wGGbWyZXyrf0/7KgtIj7S1kWZWeez3UP7knoAvYCa7CqyTQPVewED2qE2M+tEdvQ5o38CrqIQPLP5IIxWAreWuS4z62RK+QT2lRHxf9upnt3iT2CbJW/Xz/QIvCNpTwBJX5P0S0lD2qw0MzNKC6P/FRGrJJ0EnA7cDfy4vGWZWWdTShg1ZD//FvhxRPwan9fIzNpYKWH0lqTbgQsonPGxe4nrmZmVrJRQuQCYBoyOiOVAf+CaslZlZp3OTsMoItYC7wEnZU2bgVfKWZSZdT6lfAL7G8C/Al/NmvbAV5Q1szZWym7a3wHnAGsAImIJsMMTrpmZtVYpYbQxu2ZaAEjqXd6SzKwzKiWM7s+OpvWV9Hng98Ad5S3LzDqbUi5VtDcwmcJ30o4Avg58spxFmVnnU8p3056NiCHN2uZExIfLWtku8HfTzJLX+u+mSfqCpLnAEZLmFN1eA+aUtFVptKSXJS2QdO12lrlA0nxJ8yTdW9R+saRXstvFpWzPzDqu7faMJPUB+gHfBYqDZFVE1O/0gaUq4C/ASGAxMBMYFxHzi5Y5DLgfODUilknaJyLek9QfmAUMo9CLmQ0MjYhlO9mme0ZmaWv9ddMiYgWwAhi3ixsdDiyIiFcBJE0CxgDzi5b5PHBrU8hExHtZ++nAo02hJ+lRYDRw3y7WYmaJK+d3zA4AFhVNL87aih0OHC7pSUl/kjS6FeuaWQUp5WjarmqpO9Z896grcBhwClAL/LekY0pct7AR6TLgsmyyZpcqNbPclbNntBg4sGi6FljSwjK/johNEfEa8DKFcCplXQAiYmJEDMvGit5vq+LNrH2VM4xmAodJGiSpGzAWmNJsmYeATwBIqqGw2/YqhbMEjJLUL7sYwKiszcwqVNnCKCI2A1dQCJGXgPsjYp6kCZLOyRabBtRJmg/8F3BNRNRlA9c3UAi0mcCEUo7gdXarV69m2rRpvP7663mXYtZqO/3QY0fSmQ/tz549m5EjR7Js2TK6dOnCTTfdxJe+9KW8yzJrrvWH9q18ZsyYQV1dXZs+5re+9S2WLSt8DKuxsZHrrruO2tpaevXq1WbbqK6u5qMf/WibPZ5ZMZ8+tkI0BVGTjRs3smbNmpyqMWs994xyUI7excKFC7n66qu3TH/84x/nc5/7XJtvx6xcHEYV4qqrrqJfv35MnDiR2tpabr/99rxLMmsVh1EFufjii+nfvz8Affv2zbkas9bxmJGZJcFhZGZJcBiZWRIcRhVm7dq1NDY25l2GWat5ALtCvPPOO1x44YU88cQT1NTU0LNnT0aNGpV3WWYl89dBKsRnP/tZ7rnnni3Te++9N4sWLaJ79+45VmW2jdafA9s6lueee26r6aVLl7J48eKcqjFrPYdRhTjttNO2mj7kkEMYNGhQTtWYtZ7HjCrEjTfeyLp165g8eTK1tbX8/Oc/p0sX/6+xjsNjRhXm4YcfBuDss8/OuRKzFnnMqLNYu3YtTz75JI8//jiV9I/GKp97RhXktddeY+jQoVtOJ3Leeefx4IMP5lyV2VbcM6pEDz/8MNdddx2PPPIIALfccstW5zX65S9/ycyZM/Mqz6xVPIC9A+U4I2Nbuffee5k0adKW6YsuuoglS7a9gMpvf/tb3nnnnfYsrdV8BkkD94x2qK6uLtkwmjJlyjbTI0eO3OoI2gEHHMBf//Vft3dprZLya2ztyz2jnaiurk7yyFTPnj1Zu3btlunevXtzxhln8MYbb/D6668zYsQILr/8cvbee+8cq9y5pqN/nV1s3MC66VNpWPouPU44hT0OOTzvktqdw6iD+trXvrbVaWbPPvtshgwZsuVLsgMGDEg+iOwD9RO+xMbnnwFg9f0/of+EH9H92L/Juar25TDqoK666ipOOOEEnnrqKU488USuvvrqrb6t/9Of/pTvfve77LPPPjlWWZnaeiyxR/17HJMFEQANDSy840csPP2CNtsGpD825zDqwEaMGMGIESPyLsN2l7Yduo1O+Ol5h1GF+PKXv8xTTz21pXc0fvx494rKpBy9i2WLX2b9jMcKE927c9j/+DJHH5n2wYe25jCqEOeeey7PPvssjzzyCEcddRRjxozJuyRrhb7/eiN/vO1muq1awdB//AJd9x2Qd0ntzmFUQY499liOPfbYvMuwXaCqKpYffARApwwi8OeMzCwRDiMzS4J303ZgxYoVbN682R/MK6O6ujq6dvXb0NwzMrNE+F/SDvTp0wfwicrKyb1Oa+KekZklwWFkZklwGFWQn/zkJ5x33nlcf/31LF++PO9yzFrFY0YV4uabb+aLX/wiAL/61a948sknmT59er5FmbWCe0YVovhqsgCPP/44b775Zk7VmLWew6hC7LfffltN9+zZk759++ZUjVnrOYwqxA033ED//v0B6NKlC9/+9rfZa6+9cq7KrHQeM6oQQ4cO5c0332TGjBkcfvjhDBw4MO+SzFrFYVRBevfuzciRI/Muw2yXlHU3TdJoSS9LWiDp2hbmj5e0VNLz2e3SonkNRe1Tmq9rZpWlbD0jSVXArcBIYDEwU9KUiJjfbNFfRMQVLTzEuog4rlz1mVlaytkzGg4siIhXI2IjMAnw6QfNrEXlDKMDgEVF04uztuY+JWmOpMmSDixq7yFplqQ/STp3exuRdFm23Cygpm1KN7P2Vs4wUgtt0Wz6YeDgiPgw8Hvg7qJ5B0XEMODvgVskHdrSRiJiYkQMy5Z9vw3q7hAaGhp499138y7DrM2UM4wWA8U9nVpgq4vBR0RdRGzIJu8AhhbNW5L9fBWYDhxfxlo7lD/+8Y8ccsgh7LfffhxzzDH8+c9/Bgqfur7mmmu466672LRpU85VmrVOOQ/tzwQOkzQIeAsYS6GXs4Wk/SPi7WzyHOClrL0fsDYiNkiqAU4EbipjrR1GRDB+/PgtX/WYN28eV1xxBZdccgkXXXTRluWmTp3K/fffn1eZZq1WtjCKiM2SrgCmAVXAXRExT9IEYFZETAH+RdI5wGagHhifrX4UcLukRgq9t++1cBSuXdTV1SV1ArD169ezcOHCrdpmzZrFokWLtmqbPHkyd99995ZPZaeqrq6O6urqvMuwBJT1Q48R8Z/AfzZr+3rR/a8CX21hvRlA7lewS/GPpEePHgwePJj58z/I5iFDhvD++1sPl3Xp0qVDnFu6uro6yde5nDbMmc3q+39CbFhP7789n56njM67pCSk/27NUarXJT/++OO56qqrmD17Nqeddho/+MEPePrppznrrLO2jBVdeeWVfOYzn8m5Umuu4b13qP/G/4RNGwFY/tIcuvSvgQgGP3A73VavYPnrc+nzT19C3brnXG37UkTzA1wdl6RZ2VG1namcJ13ktdde4/vf/z4HHnggX/3qNh3OTmfGjBnU1dXlXcZWal56joOf+M1Wbe8ePYzqV+bSdeOGLW1vDT2Zt4d9vL3L2yXV1dWt+cfd0lF2wD2jijJo0CDOOOOMvMtIRl1dXXJjUuv7bltLY9c9tgoigD3ffpO3t1kyPW0Z9g4jq2jV1dXJXd1lpTaw5uEHoLGB7h85mWOvvI6ll55HrFu7ZZn9TzyFIxKruyVteXDHYWTWzvb6/BfpfcElsGkjVTX7AtD3mhtYefv/oWHpe/Q46TQ+dOElOVfZ/hxGZjmo6tNvq+kewz9Gj+EfIxobUZfOec7DzvmszRLVWYMIHEZmlgiHkZklwWFkZklwGJlZEnw0zaydbXjxWdY8dC8E9B4zlu4fHkbj6pWs+c0DNCx9l54f+yTdjxued5ntzmFk1o42v/UG9V+7EjYXvkO4YfYMan54DytumcCmV14CYN20h+h3/U30OOGUHCttfw4jq1grVqxg8+bNSZ0CZr/nZ1C7uejEd5s3M++2H7BvFkRNXrvnDha8v6qdq2u9urq6Njs7hMeMzNrRhj37bNu2V79t2ho62Tf2wT0jq2B9+hT+8FP6blo0nMGyNfVseGo6AN2Hf4zh//otVt6+J2t/+0sA9KG9OOKq6znm4BZP+54UfzfNrINSVVf6X38Tm996AwK61hYuQ97n8mvp+cmzaVj6Nt2P/whden8o50rbn8PILAddDxi4TVu3I46GI47OoZo0OIw6qIaGBn73u9+xZs0azjzzTHr16pV3SWa7xWHUATU0NHDqqafyxBNPADBw4ECefvpp9t1335wrM9t1PprWAU2dOnVLEAG88cYbTJw4MceKzHafe0Yd0MqVK7dpq6+v5/LLL+eBBx6gtraWQw89lMGDB+dQndmucc+oAzrrrLOora3dMt2jRw/q6uq47bbbWLp0Kc899xxjxoyhsbExxyrNWsc9oxy0xVUrbrjhBqZOncr69eu3XK6o2IIFC7jzzjvZf//9d2s7xVp5FQizVnEYdVDV1dVbXRdt0KBBvPHGG1um99prL2pqavIozWyXOIxyUI7exdChQ/n0pz/NjBkzGDBgAHfccQdnnnlmm2/HrFwcRhViwIABPPnkk9TX19OnTx+qqqryLsmsVRxGFaZ///55l2C2S3w0zcyS4DAysyQ4jMwsCQ4jM0uCw8jMkuAwMrMkOIzMLAkOIzNLgsPIzJLgMDKzJDiMzCwJDiOznG2c/wLrn/lvYuOGvEvJVVnDSNJoSS9LWiDp2hbmj5e0VNLz2e3SonkXS3olu11czjrN8lL/7Wuo+8rnWTbhSyz9wgU01L+fd0m5KVsYSaoCbgXOAAYD4yS1dFLmX0TEcdntzmzd/sA3gBHAcOAbkra9BrBZB7bxxefY8KfHt0w3vPs2a35zf44V5aucPaPhwIKIeDUiNgKTgDElrns68GhE1EfEMuBRYHSZ6jTLRePqbS+sECtX5FBJGsoZRgcAi4qmF2dtzX1K0hxJkyUd2Mp1zTqs7sePoEvNPh80dKmi52l/m19BOSvnydXUQls0m34YuC8iNkj6Z+Bu4NQS1y1sRLoMuCyb9EmfLXnrn36C1Q/eA8Ce4y5l81tv0rD0XRrXr2XVz26n58dG0mv0uTlX2f7KGUaLgQOLpmuBJcULRETxJTLuAP530bqnNFt3eksbiYiJwEQASbN2p2CrPHV1dTz88MN5l7FFz7p3GfzgHSgK/1uXvzSH+X/3OQ59YRY9Vi4DYOMLM3lh7hzeP/L4PEstSV1dHdXV1W3yWOXcTZsJHCZpkKRuwFhgSvECkoqvo3MO8FJ2fxowSlK/bOB6VNZmVrLq6uo2+0NpK33eXLAliAAUwd5/fm5LEDXpt3B+e5e2S9ryNS5bzygiNku6gkKIVAF3RcQ8SROAWRExBfgXSecAm4F6YHy2br2kGygEGsCEiKgvV61WmVK8xtv6mj1Z9sxjW7X91amjWfnnF6CxYUvb/sccy5Fnn93e5eVKES0OxXRIkmZFxLASFq2cJ20dSjQ2suLfv8O63/8GgJ6nnkmfK69nzS9/xop7fkyXxkaqDjiI6hv+nap99su52rJoaTy4MMNhZNb+GpbXQwRV/T7YxZk66V72WLuaU8dfirpU7JcjthtGvlSRWQ6q+m57SalNvfdkU+89KzmIdqhzPmszS47DyMyS4DAysyQ4jMwsCQ4jM0uCw8jMkuAwMrMkOIzMLAkOIzNLgsPIzJLgMDKzJDiMzCwJ/qKsWSK6rVpOtzWriM2bUdfO96fpU4iYJWDVzyeyatL/QxFU7bs//W+8ja77VeQ1KHw+o2Yq50lbu5sxYwZ1dXU7X7BEe6xeyYfv/dFWp6NdeuRxvPHxtj3TY3V1dQpnv9xuGHnMyCxne6xdvVUQAXRbvSqnavLT+XZMzXZTW/cuoqGBpc88SsNbb25pO+jTn+HI085q0+2kzrtpZgloeP9dVj9wNw3vvU2Pk0fR6xNn5F1SuXjMqJnKedJmHYvHjMwsbQ4jM0uCw8jMkuAwMrMkOIzMLAkOIzNLgsPIzJLgMDKzJDiMzCwJDiMzS4LDyMyS4DAysyRU2ilE3i9xue1+Wc/M8lFR39o3s47Lu2lmlgSHkZklwWFkZklwGJlZEhxGZpYEh5GZJcFhZGZJcBiZWRIq7RPYJZE0FajJu44yqqH0T6NbWir9d/d+RIxuaYY/gV2BWnH9OEtMZ/7deTfNzJLgMDKzJDiMKtPEvAuwXdZpf3ceMzKzJLhnZGZJcBhVEEmvSTqiWdstkr5SNP1DSW9J6lLUNl7SUknPF90Gt2ftHYGkn0o6P7t/Z9NrJOm6ZsvNKOe22/Axvynpu83ajpP0UtH08ZJC0unNlmto9n65dnfrcRhVlknA2KaJLHDOB35RNP13wCLg5Gbr/iIijiu6zW+nmjukiLi06DW6rtm8j+ZQ0q64D7iwWdtY4N6i6XHAH7OfxdY1e798b3eLcRglRtLBkv4s6W5JcyRNltRL0uuSviPpKUmzJA2RNE3SQkn/nK1+H0VhRCFwXo+IN7LpTwAvAj9m2zdXRZH02ez1e0HSPVnbQEl/yNr/IOmgrP2nkn4kaYakV4t6P5L075LmS3oE2Kfo8adLGibpe0DPrHfw82ze6qL1vy/pRUlzJV2YtZ+SrT85+13/XJKyeV+XNDNbZ2JT+w6e5/Ss9zsjW2d41v7N7D30u+y9c56km7I6pkraIyJeBpZLGlH0kBdQ+KdGtu3zgfHAKEk9dvPXsmMR4VtCN+BgIIATs+m7gC8DrwNfyNpuBuYAewJ7A+8VrT8PODa7/x/A5UXz7gT+AdgLeAvYI2sfDywFni+69cz7tdiN1/Bo4GWgJpvun/18GLg4u/854KHs/k+BByj8cx4MLMjazwMeBaqAAcBy4Pxs3nRgWHZ/dbPtr85+fqpo/X2BN4H9gVOAFUBtts2ngJOKa83u3wOcXVTj+S081+nAHdn9k4EXs/vfpNCj2QM4FlgLnJHN+xVwbnb/GuDm7P5HgJlFj30S8Ifs/r3AeUXzGpq9Xy7c3d+be0ZpWhQRT2b3f0bhTQEwJfs5F3g6IlZFxFJgvaS+2bz7gLGSugJjKPyRIakbcCaFP8CVwNPAqKJtNt9NW1e2Z1d+pwKTI+J9gIioz9pP4INdkHv44HWFwuvSGIVdr32ztpOB+yKiISKWAI+1so6TitZ/F3gc+Jts3jMRsTgiGin8MR+ctX9C0tOS5mbP4+gStnNf9jyfAPYqei/8NiI2UXi/VAFTs/a5RdubBJyf7cKPbXqszLhsftNyxb3p5rtpvyihzh3qlN9N6wCaf96iaXpD9rOx6H7TdNPv8j7gdxTe+HMi4r2sfTTQB5ib9fx7Ufhv+UibVp4Gse1r2JLiZYpfT21nmV2pY3uKt9cAdM12g26j0ONaJOmbQCm7Rjt8v0REo6RNkXVpKHq/ZNt5Hfg4hZ7cCQCSqrLpcyRdnz2Xakl7RsSqEmpqNfeM0nSQpBOy+00DiCWJiIVAHfA9tv0vd2lEHBwRBwODKIwD9GqbkpPyB+ACSdUAkvpn7TP4YEztM+z8dX2CQi+zStL+FMbcWrJJ0h7bWf/CbP29KfS0ntnB9pqC531JH6IwXlOKprGok4AVEbGixPWa3Edh139hRCzO2j4JvBARB2bvmYHAg8C5rXzskjmM0vQScLGkOUB/CgPOrXEfcCSFsQGywDmdol5QRKyh8Md4dtZ0YbNDtR3liNA2ImIecCPwuKQXgH/LZv0LcEn2uv4D8D938lC/Al6hsFvzYwq9zZZMBOY0DWA3W38O8AKFXbyvRMQ7O6h7OXBHtr2HgJk7qa/JMhU+TvAfwD+WuE6xByjsDk4qahtH9v4p8iDw99n9ns3eL7t9NM2fwE6MpIOB30TEMfdtEqwAAAA9SURBVDmXYh2ApOnAlyNiVt617C73jMwsCe4ZmVkS3DMysyQ4jMwsCQ4jM0uCw8jMkuAwMrMkOIzMLAn/H3IN/7kDxt6DAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "plt.figure(figsize=(4,6))\n",
    "ax = sb.swarmplot(data=results_df,x='model',y='test_mse',hue='model',palette=['black','#E64B35FF'])\n",
    "sb.boxplot(data=results_df,x='model',y='test_mse',palette=['white','white'])\n",
    "# plt.ylim([500,975])\n",
    "ax.get_legend().remove()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "for loc, spine in ax.spines.items():\n",
    "    if loc in ['left','bottom']:\n",
    "        spine.set_position(('outward', 10))\n",
    "# plt.savefig('figures/mlp_multiplicative_corr_group_ensemble_panel_Fig3.pdf')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,6))\n",
    "ax = sb.swarmplot(data=results_df,x='model',y='ari',hue='model',palette=['black','#E64B35FF'])\n",
    "sb.boxplot(data=results_df,x='model',y='ari',palette=['white','white'])\n",
    "# plt.ylim([500,975])\n",
    "ax.get_legend().remove()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "for loc, spine in ax.spines.items():\n",
    "    if loc in ['left','bottom']:\n",
    "        spine.set_position(('outward', 10))\n",
    "# plt.savefig('figures/mlp_multiplicative_corr_group_ensemble_panel_Fig3.pdf')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
